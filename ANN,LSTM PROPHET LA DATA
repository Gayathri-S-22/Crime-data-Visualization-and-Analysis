ANN
import pandas as pd 
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense
from sklearn.metrics import mean_squared_error
from scipy.stats import spearmanr
​
# load the LA crime data into a DataFrame
la_crime_data = pd.read_csv('LOS_ANGELES.csv',error_bad_lines=False)
​
# convert date columns to datetime format
la_crime_data['DATE OCC'] = pd.to_datetime(la_crime_data['DATE OCC'])
la_crime_data['Date Rptd'] = pd.to_datetime(la_crime_data['Date Rptd'])
​
# summarize the number of crime incidents per day
la_crime_data = la_crime_data.groupby(pd.Grouper(key='DATE OCC', freq='D')).size().reset_index(name='Incidents')
​
# normalize the data using MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
la_crime_data['Incidents'] = scaler.fit_transform(la_crime_data['Incidents'].values.reshape(-1, 1))
​
# split the data into training and testing sets
train_size = int(len(la_crime_data) * 0.8)
train, test = la_crime_data[:train_size], la_crime_data[train_size:]
​
# create input features and output variable
look_back = 3
train_X, train_Y = [], []
test_X, test_Y = [], []
for i in range(look_back, len(train)):
    train_X.append(train.iloc[i-look_back:i, 1].values)
    train_Y.append(train.iloc[i, 1])
for i in range(look_back, len(test)):
    test_X.append(test.iloc[i-look_back:i, 1].values)
    test_Y.append(test.iloc[i, 1])
​
# reshape input features to 2D array
train_X = np.array(train_X)
train_Y = np.array(train_Y)
test_X = np.array(test_X)
test_Y = np.array(test_Y)

# create ANN model
model = Sequential()
model.add(Dense(64, input_dim=look_back, activation='relu'))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')

# fit the model
model.fit(train_X, train_Y, epochs=100, batch_size=1, verbose=2)

# make predictions on the test data
test_predict = model.predict(test_X)

# inverse transform the predictions and actual values to get original scale
test_predict = scaler.inverse_transform(test_predict)
test_Y = scaler.inverse_transform(test_Y.reshape(-1, 1))

# calculate RMSE and spearman correlation
rmse = np.sqrt(mean_squared_error(test_Y, test_predict))
corr, _ = spearmanr(test_Y, test_predict)

print("RMSE:", rmse)
print("Spearman correlation:", corr)

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('LOS_ANGELES.csv', usecols=['DR_NO', 'Date Rptd', 'DATE OCC', 'TIME OCC', 'AREA ', 'AREA NAME', 'Rpt Dist No', 'Part 1-2', 'Crm Cd', 'Crm Cd Desc', 'Mocodes', 'Vict Age', 'Vict Sex', 'Vict Descent', 'Premis Cd', 'Premis Desc', 'Weapon Used Cd', 'Weapon Desc', 'Status', 'Status Desc', 'Crm Cd 1', 'Crm Cd 2', 'Crm Cd 3', 'Crm Cd 4', 'LOCATION', 'Cross Street', 'LAT', 'LON'])

# Handle missing data
df = df.dropna()

# Encode categorical variables
categorical_vars = ['Crm Cd Desc', 'Mocodes', 'Vict Sex', 'Vict Descent', 'Premis Desc', 'Weapon Desc', 'Status Desc', 'LOCATION', 'Cross Street']
encoder = LabelEncoder()
for var in categorical_vars:
    df[var] = encoder.fit_transform(df[var])

# Scale numerical variables
numerical_vars = ['Vict Age', 'LAT', 'LON']
scaler = StandardScaler()
df[numerical_vars] = scaler.fit_transform(df[numerical_vars])

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop('Crm Cd Desc', axis=1), df['Crm Cd Desc'], test_size=0.2, random_state=42)


LSTM
from keras.layers import Dense, LSTM
import pandas as pd 
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense
from sklearn.metrics import mean_squared_error
from scipy.stats import spearmanr
# convert date column to datetime format
df=pd.read_csv("LOS_ANGELES.csv")
df['DATE OCC'] = pd.to_datetime(df['DATE OCC'])
​
# summarize the number of crime incidents per day
df = df.groupby(pd.Grouper(key='DATE OCC', freq='D')).size().reset_index(name='Incidents')
​
# normalize the data using MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
df['Incidents'] = scaler.fit_transform(df['Incidents'].values.reshape(-1, 1))
​
# split the data into training and testing sets
train_size = int(len(df) * 0.8)
train, test = df[:train_size], df[train_size:]
​
# create timesteps for LSTM
def create_timesteps(data, look_back=1):
    X, Y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:(i + look_back), 0])
        Y.append(data[i + look_back, 0])
    return np.array(X), np.array(Y)
​
# create input features and output variable
look_back = 3
train_X, train_Y = create_timesteps(train[['Incidents']].values, look_back)
test_X, test_Y = create_timesteps(test[['Incidents']].values, look_back)
​
# reshape input features to 3D array
train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))
test_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], 1))
​
# create LSTM model
model = Sequential()
model.add(LSTM(50, input_shape=(look_back, 1)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
​
# fit the model
model.fit(train_X, train_Y, epochs=100, batch_size=1, verbose=2)
​
# make predictions on the test data
test_predict = model.predict(test_X)
​
# inverse transform the predictions and actual values to get original scale
test_predict = scaler.inverse_transform(test_predict)
test_Y = scaler.inverse_transform([test_Y])
​
# calculate RMSE and spearman correlation
from sklearn.metrics import mean_squared_error
from scipy.stats import spearmanr
​
rmse = np.sqrt(mean_squared_error(test_Y[0], test_predict[:,0]))
corr, _ = spearmanr(test_Y[0], test_predict[:,0])
​
print("RMSE:", rmse)
print("Spearman correlation:", corr)
PROPHET MODEL
import pandas as pd
from prophet import Prophet
from sklearn.metrics import mean_squared_error
from scipy.stats import spearmanr
​
# Load data and keep necessary columns
df = pd.read_csv('LOS_ANGELES.csv', usecols=['DATE OCC'])
df.columns = ['ds']
​
# Convert date columns to datetime
df['ds'] = pd.to_datetime(df['ds'])
​
# Aggregate crime count by date
df = df.set_index('ds').resample('D').size().reset_index()
df.columns = ['ds', 'y']
​
# Set the capacity to the maximum value in your dataset
cap = df['y'].max()
​
# Add the 'cap' column to your dataframe
df['cap'] = cap
​
# Train-test split
train_size = int(0.8 * len(df))
train_df = df[:train_size]
test_df = df[train_size:]
​
# Hyperparameter tuning
seasonality_prior_scale = [0.01, 0.1, 1.0, 10.0]
changepoint_prior_scale = [0.001, 0.01, 0.1, 1.0]
growth = ['linear', 'logistic']
best_rmse = float('inf')
best_spearman = float('-inf')
for sps in seasonality_prior_scale:
    for cps in changepoint_prior_scale:
        for gr in growth:
            # Fit Prophet model
            model = Prophet(growth=gr, seasonality_prior_scale=sps, changepoint_prior_scale=cps)
            model.fit(train_df)
​
            # Make predictions on test set
            future = model.make_future_dataframe(periods=len(test_df))
            future['cap'] = cap # Add 'cap' column to future dataframe
            forecast = model.predict(future)[train_size:]
​
            # Calculate RMSE
            rmse = mean_squared_error(test_df['y'], forecast['yhat'], squared=False)
​
            # Calculate Spearman's correlation
            correlation, pvalue = spearmanr(test_df['y'], forecast['yhat'])
​
            # Keep track of best RMSE and Spearman correlation
            if rmse < best_rmse:
                best_rmse = rmse
                best_spearman = correlation
​
print('Best RMSE:', best_rmse)
print('Best Spearman correlation:', best_spearman)

